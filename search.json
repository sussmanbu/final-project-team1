[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nOur primary data set comes from data.boston.gov and contains data regarding shootings in the Boston area. Data was collected by the Boston Regional Intelligence Center under the Boston Police Department Bureau of Intelligence and Analysis. This data includes both fatal and non-fatal shootings (shooting_type_v2), if a victim was struck by a bullet within the City of Boston that falls under the jurisdiction of the Boston Police Department (district). The data does not include shootings that were deemed justifiable or self-inflicted gunshot wounds. The dataset serves as a benchmark for BPD to analyze safety and violence in Boston districts and to assess the allocation of their focus and resources. It was collected with the aim of monitoring and responding to shooting incidents within the city. The data is from 2015 forward. The data set is comprised of 1925 rows and 8 columns, labeled “incident_num”, “shooting_date”, “district”, “shooting_type_v2”, “victim_gender”, “victim_race”, “victim_ethnicity_NIBRS”, and “multi_victim”.\nload_and_clean_data.R has our cleaned data set.\nWe used the tidycensus package to add census data from the American Community Survey to our original data set. We are considering key indicators such as employment status, percentage of those above the poverty level, unemployment rate, median income, percent of single-parent households, high school graduate population, and percentage of homes that are owner occupied. We have also pulled demographic data and geolocation data to help analyze the distribution of incidents across races between Boston districts. The data tables will be merged based on each year of our initial shooting data set.\nHere is a slightly simplified version of the code we used to combine the tidycensus data with our original data set.\nlibrary(tidycensus)\nlibrary(tidyverse)\nshooting_data &lt;- read.csv(here::here(\"dataset\", \"BostonShootingDataClean.csv\"))\nshooting_data &lt;- mutate(shooting_data, year = year(Date))\n\ncensus_api_key('edc88cbdb20f0ecb1fde3abf4e45a732dd998e96')\n\nvars&lt;-load_variables(2017, \"acs5\", cache = TRUE)\n\nfilter_vars&lt;-vars %&gt;%\n  filter(str_detect(label, \" one person\"))\n\nzips&lt;-c(2135,2121, 2122, 2124,...)\n\n\n#get acs data for each year in our data set\n\ndat_2020&lt;-get_acs(geography = 'zcta', \n                  variables = c(medincome = \"B19013_001\",\n                                hashighschooldiploma ='B15003_017E',\n                                employmentstatus = 'B23025_001E',\n                                #etc...\n                              ),\n                  \n                  zip = \"MA\", \n                  year = 2020)\n\n#filter for zipcodes in collected list of zipcodes.\n#Pivot so each variable has its own column\ndf_2020&lt;-dat_2020%&gt;%\n  mutate(GEOID = as.integer(GEOID))%&gt;%\n  filter(GEOID %in% zips)%&gt;%\n  select(-moe)%&gt;%\n  pivot_wider(names_from = variable, values_from = estimate)\n  \n#add year column\ndf_2020 &lt;- df_2020 %&gt;% mutate(year = 2020)\n\n\ndat_2021&lt;-get_acs(geography = 'zcta', \n                  variables = c(medincome = \"B19013_001\",\n                                employment_status = 'B23025_001E',\n                                #etc...\n                  \n                  zip = \"MA\", \n                  year = 2021)\n\n\ndf_2021&lt;-dat_2021%&gt;%\n  mutate(GEOID = as.integer(GEOID))%&gt;%\n  filter(GEOID %in% zips)%&gt;%\n  select(-moe)%&gt;%\n  pivot_wider(names_from = variable, values_from = estimate)\n\ndf_2021 &lt;- df_2021 %&gt;% mutate(year = 2021)\n\n## we repeat this process for each year to 2015\n\n\n#combine all the acs data into one table \ncombined_table &lt;- bind_rows(df_2015, df_2016, \n                            df_2017,df_2018, df_2019,\n                            df_2020,df_2021)\n\n\nnew_column_names&lt;-c(medincome = \"B19013_001\",employment_status = \"B23025_001E\",\n  pct_below_poverty_level = \"B17001_002E\", #etc...\n)\n\n#rename the census return variables\ndf &lt;- combined_table %&gt;%\n  rename(\n    medincome = medincome,\n    no_english = B16004_005,\n    little_english = B16004_004,\n    has_highschool_diploma = B15003_017,\n    employment_status = B23025_001,\n    pct_below_poverty_level = B17001_002,\n    pct_homes_owner_occupied = DP04_0046P,\n    total_pop = B02001_002,\n    White_alone = B02001_003,\n    Black_or_African_American_alone = B02001_004,\n    American_Indian_and_Alaska_Native_alone = B02001_005,\n    Asian_alone = B02001_006, \n    Native_Hawaiian_and_Other_Pacific_Islander_alone\n    = DP04_0046P,\n    pct_25_and_up_bachelors_degree\n    = pct_25_and_up_bachelors_degree\n  )\n\ncolnames(df)\n\n#get original data set columns\nselected_columns &lt;- shooting_data[, c(\n  \"district_name\",\n  \"incident_num\",\n  \"Date\",\n  \"Time\",\n  \"district\",\n  \"victim_gender\",\n  \"victim_race\",\n  \"multi_victim\",\n  \"v_hispanic_or_latinx\",\n  \"fatal\",\n  \"Total.\"\n)]\n\n#add year column to shooting dataset \nselected_columns$year &lt;- year(as.Date(selected_columns$Date))\n\n\n\n\nzips&lt;-c(02135,02121, 02122, 02124, ...)\n\n#add district names corresponding to zipcodes/Geoid \n#for final merge with original dataset\nwith_districts &lt;- df %&gt;%\n  mutate(district_name = case_when(\n    GEOID == 2135 ~ \"Brighton\",\n    GEOID %in% c(2121, 2122, 2124, 2125) ~ \"Dorchester\",\n    GEOID == 2128 ~ \"East Boston\",\n    GEOID == c(2136,2137) ~ \"Hyde Park\",\n    GEOID == c(2130,2135) ~ \"Jamaica Plain\",\n    GEOID == 2126 ~ \"Mattapan\",\n    GEOID %in% c(2119,2120,2132) ~ \"Roxbury\",\n    GEOID == 2127 ~ \"South Boston\",\n    GEOID %in% c(2111, 2116, 2118, 2127) ~ \"South End\",\n    GEOID == 2132 ~ \"West Roxbury\",\n    TRUE ~ NA_character_\n  ))\n\n\n\nfinal_df&lt;-merge(selected_columns,with_districts, by =\n                  c('district_name','year'))\n\nwrite.csv(final_df, file = \"census_dat.csv\", row.names = FALSE)\n\n\ncolnames(final_df)"
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones and summarize the rest.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your load_and_clean_data.R file.\nRename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your load_and_clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.Rmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this pge should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nInteractive\nYou will also be required to make an interactive dashboard like this one.\nYour Big Data page should include a link to an interactive dashboard. The dashboard should be created either using Shiny or FlexDashboard (or another tool with professor’s approval). This interactive component should in some way support your thesis from your big picture page. Good interactives often provide both high-level understanding of the data while allowing a user to investigate specific scenarios, observations, subgroups, etc.\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions from the Big Picture? Plotly with default hover text will get no credit. Be creative!\n\n\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team TEAMNAME. The members of this team are below."
  },
  {
    "objectID": "about.html#hirotaka-caden-fuiji",
    "href": "about.html#hirotaka-caden-fuiji",
    "title": "About",
    "section": "Hirotaka Caden Fuiji",
    "text": "Hirotaka Caden Fuiji\nHiro is an undergraduate student majoring in Data Science. His GitHub Page is https://github.com/fujiihc."
  },
  {
    "objectID": "about.html#john-markowicz",
    "href": "about.html#john-markowicz",
    "title": "About",
    "section": "John Markowicz",
    "text": "John Markowicz\nJohn is an undergraduate student majoring in Data Science. His GitHub page is https://github.com/jcmarkowicz."
  },
  {
    "objectID": "about.html#alexandra-rodriguez",
    "href": "about.html#alexandra-rodriguez",
    "title": "About",
    "section": "Alexandra Rodriguez",
    "text": "Alexandra Rodriguez\nAlexandra is an undergraduate student majoring in Computer Science and Psychology. Her Github page is https://github.com/lexi0001."
  },
  {
    "objectID": "about.html#nathan-rosenblum",
    "href": "about.html#nathan-rosenblum",
    "title": "About",
    "section": "Nathan Rosenblum",
    "text": "Nathan Rosenblum\nNathan is a undergraduate senior studying Applied Mathematics, his github page is https://github.com/nrosenblum."
  },
  {
    "objectID": "about.html#eric-yang",
    "href": "about.html#eric-yang",
    "title": "About",
    "section": "Eric Yang",
    "text": "Eric Yang\nEric is an undergraduate student in Economics with a minor in History. His Github page is https://github.com/ericyang02.\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "posts/Blog_6/Blog_6.html",
    "href": "posts/Blog_6/Blog_6.html",
    "title": "Blog 6: Thesis",
    "section": "",
    "text": "Our thesis is as follows: Shooting Frequency is negatively correlated with Income Per Capita. As IPC increases, Shooting Frequency decreases. We have initial graphs that support this thesis. We decided to use Income Per Capita rather than median income to normalize our values to account for differing population sizes. We are in the process of exploring the relationships between Shooting Frequency and Employment Status as well as Education. Our initial assumptions are that low employment is correlated with more shootings, and that low education (defined as having a high school diploma) is correlated with more shootings. In terms of additional exploration, we are in the process of generating graphs to either verify or refute our previous claims. We aim to investigate the relationship between socioeconomic factors and the rate of shooting incidents in the Boston area over a ten-year period from 2015 to 2024. Our goal is to understand how variables such as employment status, poverty levels, unemployment rates, median income, family structure, education levels, and homeownership rates influence the occurrence of shooting incidents across different neighborhoods. We predict that certain factors such as high unemployment rates and elevated poverty levels will correlate with increased shooting incidents in the Boston area from 2015 to 2024. Tables we might look into include using tidycensus to get visual representations of where the most police shootings occur. This will help verify whether our hypothesis of shooting frequency and location is consistent."
  },
  {
    "objectID": "posts/Blog_post_2/Blog_post_2.html",
    "href": "posts/Blog_post_2/Blog_post_2.html",
    "title": "Blog 2: Data Background",
    "section": "",
    "text": "Data Background\nData was collected by the Boston Regional Intelligence Center under the Boston Police Department Bureau of Intelligence and Analysis. This data includes both fatal and non-fatal shootings (shooting_type_v2), if a victim was struck by a bullet within the City of Boston that falls under the jurisdiction of the Boston Police Department (district). The data does not include shootings that were deemed justifiable or self-inflicted gunshot wounds. The dataset serves as a benchmark for BPD to analyze safety and violence in Boston districts and to assess the allocation of their focus and resources. It is likely collected with the aim of monitoring and responding to shooting incidents within the city. The data is from 2015 forward.\nPotential issues with this data could include inconsistencies in reporting across districts, missing or incomplete data, variation in how incidents are classified, and potential biases in reporting (underreporting in certain neighborhoods). Bias might also exist due to overrepresentation of certain demographics as victims or systemic biases within law enforcement practices.\nThis data could be used for various purposes including trend analysis over time, understanding spatial distribution across districts, examining victim demographics, and exploring relationships between shooting incidents and victim characteristics. There may have been prior research utilizing this data to understand patterns of gun violence in Boston or to inform policy decisions related to public safety. The data might inform policy decisions related to allocation of resources for crime prevention, community outreach efforts, or gun control measures. Questions from policymakers might include inquiries about trends in shootings, effectiveness of intervention programs, or factors contributing to high-risk areas."
  },
  {
    "objectID": "posts/Blog_4/blog4.html",
    "href": "posts/Blog_4/blog4.html",
    "title": "Blog 4: Data Modeling",
    "section": "",
    "text": "Two graphs we have emphasized are the relationship between victim and gender and incidents by district. Using these two graphs will help emphasize where Boston shootings most occur, and who is most at-risk in getting involved. Predictor variables we may be interested in are the district numbers and gender; these two variables are interesting to see where the greatest number of crimes occur - to observe possible patterns and for forecasting.\nOur analysis so far has highlighted gender differences and the frequency of incidents by district without exploring how these factors may interact with others. We could further explore temporal patterns to shed light on when incidents are more likely to occur. We could also focus on outcomes, particularly the distinction between fatal and non-fatal incidents, to help us understand the factors that contribute to the severity of shootings.\nBased on our dataset, there are two potential directions we can take for the choice of the response variable. We could predict the severity of incidents (whether it was fatal) or predict the frequency of incidents based on various factors (time, location, demographic information). To predict incident frequency, we would need to aggregate the data to have counts per day, district, or another relevant grouping. Possible predictor variables could include date/time for temporal trends, district name/district for geographical influences, victim race/gender for demographic factors of victims, or socioeconomic factors like income. If our response variable is the severity of incidents, logistic regression is the most appropriate for predicting the possibility of an incident being fatal based on the predictors. Linear regression could be used if our response variable is the number of incidents to fit a continuous distribution."
  },
  {
    "objectID": "posts/2024-04-29-blog-7-visualizations/blog-7-visualizations.html",
    "href": "posts/2024-04-29-blog-7-visualizations/blog-7-visualizations.html",
    "title": "Blog 7: Visualizations",
    "section": "",
    "text": "For additional graphs and tables, we may want to observe how the scale of these crime patterns may differ based on different combinations of factors. This may help emphasize which factors - median income, family structure, education levels, and homeownership rates - influence the occurrence of shooting incidents the most. We may want to emphasize the earlier years (such as 2015 - 2020), as doing so may help provide insights on how such patterns evolved, as well as helping us predict where future shootings may occur. We may also observe how factors such as population (density) affects how often there is a shooting in a particular area, as there may be more instances of shootings in areas with more people of a lower socioeconomic background. The variable of interest, income per capita, helps predict future patterns.\nDepending on the frequencies, plotly() may be useful in determining the relative frequencies of shootings based on data [Using this graph will specifically help visualize which Boston districts experience the most and least number of shootings depending ] Furthermore, packages of interest to make these plots may include ‘dplyr’. We are working on developing maps to display the distribution of demographic data over the districts for which we collected data. Following the tidycensus online text, we will plot maps for our statistically significant independent variables and incorporate our demographic data with the segregation package to map the local segregation index.\nWe are developing four models, OLS with observations for each year in our dataset, logistic regression to analyze the victim gender in each incident, a geographical weighted regression to consider the effects of neighboring locations, and an ADL time series regression to forecast incidents in 2025. Additionally, we will include scatterplots to display the interactions between statistically significant independent variables against our outcome variable."
  },
  {
    "objectID": "posts/Blog_3/Blog_3.html",
    "href": "posts/Blog_3/Blog_3.html",
    "title": "Blog 3: Data Cleaning",
    "section": "",
    "text": "Blog Post 3: Data Cleaning\nWe first used View() to get a preliminary view of the data. We found that some of the column names and values were overly complicated and could be simplified and recategorized. For example, the column “victim_ethnicity_NIBRS” contained the values “Hispanic or Latinx” and “Not Hispanic or Latinx”. This was simplified into a “v_hispanic”or_latinx” column and the values were replaced with Booleans. The “shooting_type_v2” column (values: “fatal”, “not fatal”) were reformatted into the “fatal” column and Boolean values as well.\nThe “district” column that came with the dataset only used district codes (“A7” for East Boston, “B2” for Roxbury, etc.). A list of Boston Districts and their respective codes was created with information from: https://police.boston.gov/districts/. This information was then used to create a new column “district_name” that contains the string name of the district where the shooting occurred. It is worth noting that on the website listed, A1 and A15 are listed as “A1 & A15”, “Downtown & Charlestown”; However, due to various other sources listing Downtown and Charlestown as separate districts, we separated them and mapped “A1” to “Downtown” and “A15” to “Charlestown”. Boston Police Department consistently refers to “A1” and “A15” together however, so we may remerge them in our dataset later.\nWe searched for duplicate columns and initially thought we had found them as multiple rows contained the same “incident_num”. However, upon further analysis, we found that shootings with multiple victims were split into multiple rows, as each row would correspond to each individual victim. Therefore, it would be incorrect to remove duplicates based on incident numbers. This conclusion is corroborated by the “multi_victim” column, which is true for rows in which the incident number is shared with other rows.\nIn the initial data set, date and time were combined in one column and they were both string values. To clean this column, we split the date and time into separate columns and modified the data types using the as.Date function and changed the time to match the format “%H:%M:%S” . Since the initial data set lacked demographic information for each district, we downloaded 2020 census data for each district and merged the data into our final cleaned data set. The demographic data columns of interest are household size, College/University student housing, Institutionalized population, Black, White, Hispanic or Latino, Asian, Native Hawaiian and Pacific Islander, and Other. The demographic data set did not include the district of Charlestown & Downtown resulting in a loss of 30 rows.\n\n\n\n\n\n\n\nData for Equity\nBeneficence requires maximizing benefits while minimizing harm. Applying this principle to the data involves critically evaluating the risks and benefits of collecting, analyzing, and sharing information about shootings. For example, it entails carefully considering how the data could be used to improve public safety and community well-being without reinforcing negative stereotypes about certain communities. To adhere to this principle, data should be aggregated or anonymized while protecting individuals’ identities, while still providing valuable insights for public safety efforts. Transparency is also crucial in maintaining the trust of the community from which the data was collected. It involves being clear about the collection methods, the purpose of the data, and how it will be used. Transparency also requires acknowledging the limitations of the analysis, such as potential biases in data collection (over or under reporting in certain areas) and the interpretations made from the data. Limitations in the analysis could include potential biases in how incidents are reported or recorded, leading to an incomplete or skewed understanding of crime patterns. Additionally, the data could also be misused to unfairly target specific communities."
  },
  {
    "objectID": "posts/Blog_5/Blog_5.html",
    "href": "posts/Blog_5/Blog_5.html",
    "title": "Blog 5: Second Data Set",
    "section": "",
    "text": "Our team will be modeling the rate of shooting incidents in the Boston area from 2015-2024, and to build an accurate model, our team will be analyzing socioeconomic indicators to draw meaningful conclusions. To obtain variables that we think will have significant interactions with the outcome variable, we have decided to utilize the tidy census package to query data from the American Community Survey through Census.gov tables. Specifically, we are considering key indicators such as employment status, percentage of those above the poverty level, unemployment rate, median income, percent of single-parent households, high school graduate population, and percentage of homes that are owner occupied. To introduce spatial analysis, we have pulled demographic and geolocation data for each of the years to analyze the distribution of incidents across race in key Boston districts based on zip codes. The names of each variable and their primary key in the database are listed below. \nAfter querying the relevant data, the next steps our team will take will involve merging the tables for each year with our initial data set. We plan on joining these tables based on year and zip code keys. Given that we are covering spatial data, we would like to model statistics such as diversity gradient and dissimilarity index across the different regions where incidents take place. We will also be producing map plots to demonstrate the difference in our dependent variable across the different regions of coverage. Additionally, the data is distributed over time, so our team will be able to analyze the temporal variance of incidents with a time series analysis. \nLink to R script with query code: https://github.com/sussmanbu/final-project-team1/blob/d0bbc5e22b53cb188ed9c0fa22820c64e190a2b6/tidycensus.R\nRace variables:  White alone, Black alone, American Indian and Alaska Native alone, Asian alone, Native Hawaiian and Other Pacific Islander alone\nVariable IDs:\n‘DP03_0009’  unemployment rate\n‘B19013_001’ median income \nB11005_003E single parent households\n‘B16004_005E’ percent of individuals who do no speak english at all \n‘B16004_004E’ percent of individuals who do not speak english well \n‘B15003_017E’ high school graduate population \n‘C24010_’  Occupation by industry \n‘B23025_001E’ Employment status \n‘B17001_002E’  percent of individuals below poverty level \n‘B17001_003E’ percent above poverty level \n‘DP04_0046PE’  percent of homes that are owner occupied"
  },
  {
    "objectID": "posts/2024-02-28-first-blog-post/first-blog-post.html",
    "href": "posts/2024-02-28-first-blog-post/first-blog-post.html",
    "title": "Blog 1: Dataset Proposal",
    "section": "",
    "text": "Dataset 1 - Analyze Boston Shootings\nLink: https://data.boston.gov/dataset/shootings/resource/73c7e069-701f-4910-986d-b950f46c91a1?inner_span=True\nThis dataset from Analyze Boston contains information about shootings in Boston, comprised of 1925 rows and 8 columns, labeled “incident_num”, “shooting_date”, “district”, “shooting_type_v2”, “victim_gender”, “victim_race”, “victim_ethnicity_NIBRS”, and “multi_victim”. The original collection of this data might have been driven by law enforcement or public safety organizations aiming to monitor and respond to shooting incidents within the city. The inclusion of details like shooting type, victim demographics, and whether an incident involved multiple victims suggests an intent to understand patterns in gun violence, the demographic distribution of victims, and potentially identify areas or populations at higher risk. Loading and cleaning the data is possible, and initial steps could involve identifying any missing values. The main questions this dataset could address might revolve around identifying trends in shootings over time, assessing the distribution of shootings across different districts, understanding the demographic profile of victims, and analyzing the relationship between the type of shooting incidents and victim demographics. Challenges in analyzing this dataset might include dealing with missing or incomplete data.\nDataset 2 - Tobacco Use\nLink: https://catalog.data.gov/dataset/behavioral-risk-factor-data-tobacco-use-2011-to-present\nThis data set is from the U.S. Department of Health and Human Services. This data comes from the summation of several Tobacco surveys from 2011 to 2019. It contains 15 columns after cleaning and 43341 rows detailing location, year, race, type of tobacco consumption, gender, race, age, education, and more. This data could help us understand what demographics and locations have worse tobacco use habits, and how different age groups as well as levels of education may have different trends in tobacco use. Problems may arise from using this data set if different categories have different amounts of data, it may be harder to compare trends between categories.\nDataset 3 - School Attendance\nLink : https://catalog.data.gov/dataset/school-attendance-by-student-group-and-district-2021-2022\nThis dataset analyzes public school attendance from Pre-K to twelfth grade. This range will provide us with information on how students’ backgrounds like race or income level affects whether a student attends school regularly. There are 2020 observation rows. Furthermore, because the data is separated based on district, it will be clearer on how attendance might change based on demographics. Furthermore, the dataset analyzes attendance rate as a proportion to total student count on three date ranges - 2019 to 2020; 2020 to 2021; and 2021-2022. Finally, because each school district’s attendance rate is further separated by different “categories,” like race or whether or not they qualify for “Free/Reduced Lunch,” it will be useful to analyze how different hardships that students face may correlate with attendance rate. A challenge with using this dataset might be deciding which categories to use and how we can use the categories to emphasize the most meaningful results."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nBlog 7: Visualizations\n\n\n\n\n\nIn this blog post, we discuss the potential use of additional graphs and tables to observe crime patterns based on different combinations of factors such as median income, family structure, education levels, and homeownership rates, with a focus on earlier years (2015 - 2020) to understand evolving patterns and predict future occurrences, as well as the development of models including OLS, logistic regression, geographical weighted regression, and ADL time series regression to analyze and forecast shooting incidents in Boston.\n\n\n\n\n\n\nApr 29, 2024\n\n\nHirotaka Caden Fuiji, John Markowicz, Alexandra Rodriguez, Nathan Rosenblum, Eric Yang\n\n\n\n\n\n\n  \n\n\n\n\nBlog 6: Thesis\n\n\n\n\n\nIn this blog post, we present our thesis that shooting frequency in Boston is negatively correlated with income per capita, supported by initial graphs, and discuss our exploration of the relationships between shooting frequency and employment status as well as education, aiming to investigate the impact of socioeconomic factors on shooting incidents in the city from 2015 to 2024.\n\n\n\n\n\n\nApr 22, 2024\n\n\nHirotaka Caden Fuiji, John Markowicz, Alexandra Rodriguez, Nathan Rosenblum, Eric Yang\n\n\n\n\n\n\n  \n\n\n\n\nBlog 5: Second Data Set\n\n\n\n\n\nIn this blog post, we outline our approach to modeling the rate of shooting incidents in Boston from 2015 to 2024, discussing the analysis of socioeconomic indicators and spatial data, including querying relevant variables, merging tables, and conducting spatial and time series analyses.\n\n\n\n\n\n\nApr 12, 2024\n\n\nHirotaka Caden Fuiji, John Markowicz, Alexandra Rodriguez, Nathan Rosenblum, Eric Yang\n\n\n\n\n\n\n  \n\n\n\n\nBlog 4: Data Modeling\n\n\n\n\n\nIn this blog post, we discuss the utilization of graphs to highlight the frequency of Boston shootings by district and victim gender, potential predictor variables such as district numbers and gender, and potential directions for analysis including exploring temporal patterns, outcomes, and choices for response variables such as incident severity or frequency.\n\n\n\n\n\n\nApr 8, 2024\n\n\nHirotaka Caden Fuiji, John Markowicz, Alexandra Rodriguez, Nathan Rosenblum, Eric Yang\n\n\n\n\n\n\n  \n\n\n\n\nBlog 3: Data Cleaning\n\n\n\n\n\nIn this blog post, we detail the process of cleaning the Boston shooting dataset, including simplifying column names, categorizing values, handling duplicates, and integrating demographic information, while also discussing principles of equity in data analysis such as beneficence and transparency.\n\n\n\n\n\n\nApr 1, 2024\n\n\nHirotaka Caden Fuiji, John Markowicz, Alexandra Rodriguez, Nathan Rosenblum, Eric Yang\n\n\n\n\n\n\n  \n\n\n\n\nBlog 2: Data Background\n\n\n\n\n\nIn this post, we discuss the Boston shooting dataset, covering its collection, potential biases, and how it can help understand safety and violence in the city.\n\n\n\n\n\n\nMar 18, 2024\n\n\nHirotaka Caden Fuiji, John Markowicz, Alexandra Rodriguez, Nathan Rosenblum, Eric Yang\n\n\n\n\n\n\n  \n\n\n\n\nBlog 1: Dataset Proposal\n\n\n\n\n\nIn this blog post, we explore three diverse datasets including Boston shootings, tobacco use trends, and school attendance patterns, each offering unique insights into societal issues and demographic dynamics.\n\n\n\n\n\n\nFeb 28, 2024\n\n\nHirotaka Caden Fuiji, John Markowicz, Alexandra Rodriguez, Nathan Rosenblum, Eric Yang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "Analysis_page.html",
    "href": "Analysis_page.html",
    "title": "Analysis page",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sandwich)\nlibrary(lmtest)\n\nLoading required package: zoo\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(car)\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nfinal_df&lt;-read.csv('census_dat.csv')\n\n#shows correlation between income and total cases for each district. Lower income =&gt; more cases \n\n#income plot\ncorrelation_coef &lt;- final_df%&gt;%\n  group_by(district_name)%&gt;%\n  summarise(medincome_mean= mean(medincome), total_cases = sum(!duplicated(incident_num))) %&gt;%\n  summarise(correlation_coef = round(cor(total_cases, medincome_mean),2))\n\nfinal_df%&gt;%\n  group_by(district_name)%&gt;%\n  select(year, district_name, medincome, incident_num)%&gt;%\n  summarise(medincome_mean= mean(medincome), total_cases = sum(!duplicated(incident_num))) %&gt;%\n  distinct()%&gt;%\n  ggplot(aes(district_name, medincome_mean,fill = total_cases))+\n  geom_bar(stat= 'identity')+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+\n  labs(y = 'Avg median income',title = 'Avg Median Income and Total Cases Over All Years')+\n  annotate(\"text\", x = Inf, y = Inf, label = paste(\"Correlation coefficient: \", correlation_coef), hjust = 1.5, vjust = 1)\n\n\n\n#scatterplot with line of best fit \nfinal_df%&gt;%\n  group_by(district_name)%&gt;%\n  select(year, district_name, medincome, incident_num)%&gt;%\n  summarise(medincome_mean= mean(medincome), total_cases = sum(!duplicated(incident_num))) %&gt;%\n  distinct()%&gt;%\n  ggplot(aes( medincome_mean,total_cases))+\n  geom_point()+\n  geom_smooth(method ='lm')+\n  labs(y = 'Avg median income',title = 'Avg Median Income and Total Cases Over All Years')+\n  annotate(\"text\", x = Inf, y = Inf, label = paste(\"Correlation coefficient: \", correlation_coef), hjust = 1.5, vjust = 1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n#Highschool plot\nfinal_df%&gt;%\n  group_by(district_name)%&gt;%\n  select(year, district_name, has_highschool_diploma, Total.,incident_num)%&gt;%\n  summarise(district_name=district_name,high_school_pct= mean(has_highschool_diploma/Total.), total_cases = sum(!duplicated(incident_num))) %&gt;%\n  distinct()%&gt;%\n  ggplot(aes(district_name, high_school_pct,fill = total_cases))+\n  geom_bar(stat= 'identity')+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+\n  labs(y = 'Pct of population with high school diploma',title = 'Avg Populaion with HS Diploma and Total Cases Over All Years')\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n`summarise()` has grouped output by 'district_name'. You can override using the\n`.groups` argument.\n\n\n\n\ncorrelation_coef &lt;- final_df%&gt;%\n  group_by(district_name)%&gt;%\n  summarise(employment_pct= mean(employment_status/Total.), total_cases = sum(!duplicated(incident_num)))%&gt;%\n  summarise(correlation_coef = round(cor(total_cases, employment_pct),2))\n\n#employment plot\nfinal_df%&gt;%\n  group_by(district_name)%&gt;%\n  summarise(employment_pct= mean(employment_status/Total.), total_cases = sum(!duplicated(incident_num))) %&gt;%\n  ggplot(aes(district_name, employment_pct,fill = total_cases))+\n  geom_bar(stat= 'identity')+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+\n  labs(y = 'Pct of population employed',title = 'Avg Employment pct and Total Cases Over All Years')+\n  annotate(\"text\", x = Inf, y = Inf, label = paste(\"Correlation coefficient: \", correlation_coef), hjust = 1.5, vjust = 1)\n\n\n\n#total cases for districts plot\n\nfinal_df%&gt;%\n  group_by(district_name)%&gt;%\n  summarise(total_cases = sum(!duplicated(incident_num))) %&gt;%\n  ggplot(aes(district_name,total_cases))+\n  geom_bar(stat= 'identity')+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+\n  labs(title = 'Total Cases by District')\n\n\n\n#single parent household plot\ncorrelation_coef &lt;- final_df%&gt;%\n  group_by(district_name)%&gt;%\n  summarise(single_household = mean(single_parent_household_under_18/ Total.),total_cases = sum(!duplicated(incident_num))) %&gt;%\n  summarise(correlation_coef = round(cor(single_household, total_cases),2))\nfinal_df%&gt;%\n  group_by(district_name)%&gt;%\n  summarise(single_household = mean(single_parent_household_under_18/ Total.),total_cases = sum(!duplicated(incident_num))) %&gt;%\n  ggplot(aes(district_name,single_household, fill = total_cases))+\n  geom_bar(stat= 'identity')+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+\n  labs(title = 'Pct of Households with Single Parent')+\n  annotate(\"text\", x = Inf, y = Inf, label = paste(\"Correlation coefficient: \", correlation_coef), hjust = 1.5, vjust = 1)\n\n\n\n#poverty level plot\nfinal_df%&gt;%\n  group_by(district_name)%&gt;%\n  summarise(pct_below_poverty_level= mean(pct_below_poverty_level/Total.), total_cases = sum(!duplicated(incident_num))) %&gt;%\n  ggplot(aes(district_name, pct_below_poverty_level,fill = total_cases))+\n  geom_bar(stat='identity')+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+\n  labs(y = 'Avg pct below poverty level ',title = 'Avg pct below poverty level over All Years')\n\n\n\nlm_dat&lt;-final_df%&gt;%\n  group_by(district_name, year)%&gt;%\n  summarise(no_english_pct = mean(no_english/Total.), black_pct =mean(Black_or_African_American_alone/Total.), \n            pct_below_poverty_level=mean(pct_below_poverty_level/Total.),medincome_mean= mean(medincome),\n            mean_high_school_pct =mean(has_highschool_diploma/Total.), employment_pct = mean(employment_status/Total.),\n            total_cases = sum(!duplicated(incident_num)),single_household = mean(single_parent_household_under_18/ Total.))\n\n`summarise()` has grouped output by 'district_name'. You can override using the\n`.groups` argument.\n\nols&lt;-lm(total_cases~ district_name+ medincome_mean+ pct_below_poverty_level+employment_pct, data = lm_dat)\ncoeftest(ols, vcov. = vcovHC)\n\n\nt test of coefficients:\n\n                              Estimate  Std. Error t value  Pr(&gt;|t|)    \n(Intercept)                 1.4734e+01  1.6024e+01  0.9195 0.3627337    \ndistrict_nameDorchester     2.8293e+01  1.1878e+01  2.3820 0.0215058 *  \ndistrict_nameEast Boston    1.9820e-01  3.6226e+00  0.0547 0.9566115    \ndistrict_nameJamaica Plain  1.5009e+01  3.6411e+00  4.1220 0.0001593 ***\ndistrict_nameMattapan       4.5180e+01  4.1929e+00 10.7754 4.760e-14 ***\ndistrict_nameRoxbury        5.4235e+01  8.9295e+00  6.0737 2.427e-07 ***\ndistrict_nameSouth Boston   7.6087e+00  3.3917e+00  2.2434 0.0298422 *  \ndistrict_nameSouth End      5.7032e+00  4.6210e+00  1.2342 0.2235374    \nmedincome_mean             -1.4277e-04  1.3087e-04 -1.0909 0.2811044    \npct_below_poverty_level    -4.8695e+01  7.1871e+01 -0.6775 0.5015332    \nemployment_pct              6.7078e+00  2.8639e+01  0.2342 0.8158760    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(ols) #92 adjusted Rsquared good for forecasting\n\n\nCall:\nlm(formula = total_cases ~ district_name + medincome_mean + pct_below_poverty_level + \n    employment_pct, data = lm_dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.8233  -2.1011   0.0317   2.3574  13.0355 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 1.473e+01  2.270e+01   0.649  0.51964    \ndistrict_nameDorchester     2.829e+01  1.525e+01   1.855  0.07015 .  \ndistrict_nameEast Boston    1.982e-01  5.233e+00   0.038  0.96996    \ndistrict_nameJamaica Plain  1.501e+01  4.385e+00   3.423  0.00133 ** \ndistrict_nameMattapan       4.518e+01  7.322e+00   6.171 1.74e-07 ***\ndistrict_nameRoxbury        5.423e+01  1.176e+01   4.610 3.33e-05 ***\ndistrict_nameSouth Boston   7.609e+00  4.551e+00   1.672  0.10148    \ndistrict_nameSouth End      5.703e+00  6.409e+00   0.890  0.37825    \nmedincome_mean             -1.428e-04  1.350e-04  -1.057  0.29596    \npct_below_poverty_level    -4.870e+01  8.090e+01  -0.602  0.55026    \nemployment_pct              6.708e+00  3.390e+01   0.198  0.84403    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.727 on 45 degrees of freedom\nMultiple R-squared:  0.9413,    Adjusted R-squared:  0.9283 \nF-statistic: 72.22 on 10 and 45 DF,  p-value: &lt; 2.2e-16\n\nvif(ols)\n\n                             GVIF Df GVIF^(1/(2*Df))\ndistrict_name           456.11629  7        1.548581\nmedincome_mean           12.73971  1        3.569273\npct_below_poverty_level  29.73695  1        5.453160\nemployment_pct          124.24004  1       11.146302\n\n#interpretation: where you live is the best predictor of cases as shown by total cases by locations graph, p-values are high for other variables indicates multicollinearity\n#vif, anything above 5 high collinearity, below 1 no collinearity, above 1 medium collinearity \n\n\nols&lt;-lm(total_cases~medincome_mean+employment_pct+single_household+pct_below_poverty_level, data = lm_dat)\ncoeftest(ols, vcov. = vcovHC)\n\n\nt test of coefficients:\n\n                           Estimate  Std. Error t value  Pr(&gt;|t|)    \n(Intercept)              2.5255e+01  1.4400e+01  1.7538   0.08547 .  \nmedincome_mean           4.2569e-04  2.0927e-04  2.0342   0.04715 *  \nemployment_pct          -1.6157e+02  3.5390e+01 -4.5654 3.175e-05 ***\nsingle_household         5.6747e+02  7.5222e+01  7.5440 7.546e-10 ***\npct_below_poverty_level  2.4853e+02  1.5053e+02  1.6510   0.10488    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(ols)\n\n\nCall:\nlm(formula = total_cases ~ medincome_mean + employment_pct + \n    single_household + pct_below_poverty_level, data = lm_dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.8559  -7.6095  -0.6758   8.4018  27.7319 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              2.525e+01  1.656e+01   1.525 0.133476    \nmedincome_mean           4.257e-04  2.507e-04   1.698 0.095639 .  \nemployment_pct          -1.616e+02  3.909e+01  -4.133 0.000133 ***\nsingle_household         5.675e+02  9.411e+01   6.030 1.84e-07 ***\npct_below_poverty_level  2.485e+02  1.692e+02   1.469 0.148053    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.89 on 51 degrees of freedom\nMultiple R-squared:  0.6091,    Adjusted R-squared:  0.5785 \nF-statistic: 19.87 on 4 and 51 DF,  p-value: 6.54e-10\n\nvif(ols)\n\n         medincome_mean          employment_pct        single_household \n               7.471888               28.102173                4.273531 \npct_below_poverty_level \n              22.126258 \n\n#interpretation these variables are highly collinear, signs of the coefficients do not make sense \n#however as single households increases, cases increas,\n#as employment pct increases, cases decrease\n#check signs of estimates\n\n#check these plots for residuals, QQ, outliers\nplot(ols)\n\n\n\n\n\n\n\n\n\n\n\n\nlogit_dat&lt;-final_df%&gt;%\n  group_by(district_name, year,victim_gender)%&gt;%\n  summarise(victim_gender =victim_gender, no_english_pct = mean(no_english/Total.), black_pct =mean(Black_or_African_American_alone/Total.), \n            pct_below_poverty_level=mean(pct_below_poverty_level/Total.),medincome_mean= mean(medincome),\n            mean_high_school_pct =mean(has_highschool_diploma/Total.), employment_pct = mean(employment_status/Total.),\n            total_cases = sum(!duplicated(incident_num)),single_household = mean(single_parent_household_under_18/ Total.))%&gt;%\n  distinct()\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n`summarise()` has grouped output by 'district_name', 'year', 'victim_gender'.\nYou can override using the `.groups` argument.\n\n#logistic regression to predict victims gender, residuals highly heteroskedastic, \nlogit &lt;- glm(factor(victim_gender) ~ employment_pct+ pct_below_poverty_level+mean_high_school_pct+ medincome_mean, data = logit_dat, family = binomial)\ncoeftest(logit, vcov.=vcovHC)#heteroskedastic standard errors\n\n\nz test of coefficients:\n\n                           Estimate  Std. Error z value  Pr(&gt;|z|)    \n(Intercept)              2.6341e+00  2.3046e+00  1.1430 0.2530563    \nemployment_pct          -1.5802e+01  9.0041e+00 -1.7550 0.0792559 .  \npct_below_poverty_level  9.8619e+01  2.9291e+01  3.3668 0.0007604 ***\nmean_high_school_pct    -3.0802e+01  2.3831e+01 -1.2925 0.1961900    \nmedincome_mean           3.5031e-05  3.9856e-05  0.8789 0.3794428    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(logit)#homoskedasitic standard erros formula, pvalues much higher \n\n\nCall:\nglm(formula = factor(victim_gender) ~ employment_pct + pct_below_poverty_level + \n    mean_high_school_pct + medincome_mean, family = binomial, \n    data = logit_dat)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)              2.634e+00  7.619e+00   0.346    0.730\nemployment_pct          -1.580e+01  1.350e+01  -1.171    0.242\npct_below_poverty_level  9.862e+01  7.216e+01   1.367    0.172\nmean_high_school_pct    -3.080e+01  2.901e+01  -1.062    0.288\nmedincome_mean           3.503e-05  7.658e-05   0.457    0.647\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 19.485  on 96  degrees of freedom\nResidual deviance: 14.722  on 92  degrees of freedom\nAIC: 24.722\n\nNumber of Fisher Scoring iterations: 8"
  }
]