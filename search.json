[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nOur primary data set comes from data.boston.gov and contains data regarding shootings in the Boston area. Data was collected by the Boston Regional Intelligence Center under the Boston Police Department Bureau of Intelligence and Analysis. This data includes both fatal and non-fatal shootings (shooting_type_v2), if a victim was struck by a bullet within the City of Boston that falls under the jurisdiction of the Boston Police Department (district). The data does not include shootings that were deemed justifiable or self-inflicted gunshot wounds. The dataset serves as a benchmark for BPD to analyze safety and violence in Boston districts and to assess the allocation of their focus and resources. It was collected with the aim of monitoring and responding to shooting incidents within the city. The data is from 2015 forward. The data set is comprised of 1925 rows and 8 columns, labeled “incident_num”, “shooting_date”, “district”, “shooting_type_v2”, “victim_gender”, “victim_race”, “victim_ethnicity_NIBRS”, and “multi_victim”.\nload_and_clean_data.R has our cleaned data set.\nWe used the tidycensus package to add census data from the American Community Survey to our original data set. We are considering key indicators such as employment status, percentage of those above the poverty level, unemployment rate, median income, percent of single-parent households, high school graduate population, and percentage of homes that are owner occupied. We have also pulled demographic data and geolocation data to help analyze the distribution of incidents across races between Boston districts. The data tables will be merged based on each year of our initial shooting data set.\nHere is a slightly simplified version of the code we used to combine the tidycensus data with our original data set.\nlibrary(tidycensus)\nlibrary(tidyverse)\nshooting_data &lt;- read.csv(here::here(\"dataset\", \"BostonShootingDataClean.csv\"))\nshooting_data &lt;- mutate(shooting_data, year = year(Date))\n\ncensus_api_key('edc88cbdb20f0ecb1fde3abf4e45a732dd998e96')\n\nvars&lt;-load_variables(2017, \"acs5\", cache = TRUE)\n\nfilter_vars&lt;-vars %&gt;%\n  filter(str_detect(label, \" one person\"))\n\nzips&lt;-c(2135,2121, 2122, 2124,...)\n\n\n#get acs data for each year in our data set\n\ndat_2020&lt;-get_acs(geography = 'zcta', \n                  variables = c(medincome = \"B19013_001\",\n                                hashighschooldiploma ='B15003_017E',\n                                employmentstatus = 'B23025_001E',\n                                #etc...\n                              ),\n                  \n                  zip = \"MA\", \n                  year = 2020)\n\n#filter for zipcodes in collected list of zipcodes.\n#Pivot so each variable has its own column\ndf_2020&lt;-dat_2020%&gt;%\n  mutate(GEOID = as.integer(GEOID))%&gt;%\n  filter(GEOID %in% zips)%&gt;%\n  select(-moe)%&gt;%\n  pivot_wider(names_from = variable, values_from = estimate)\n  \n#add year column\ndf_2020 &lt;- df_2020 %&gt;% mutate(year = 2020)\n\n\ndat_2021&lt;-get_acs(geography = 'zcta', \n                  variables = c(medincome = \"B19013_001\",\n                                employment_status = 'B23025_001E',\n                                #etc...\n                  \n                  zip = \"MA\", \n                  year = 2021)\n\n\ndf_2021&lt;-dat_2021%&gt;%\n  mutate(GEOID = as.integer(GEOID))%&gt;%\n  filter(GEOID %in% zips)%&gt;%\n  select(-moe)%&gt;%\n  pivot_wider(names_from = variable, values_from = estimate)\n\ndf_2021 &lt;- df_2021 %&gt;% mutate(year = 2021)\n\n## we repeat this process for each year to 2015\n\n\n#combine all the acs data into one table \ncombined_table &lt;- bind_rows(df_2015, df_2016, \n                            df_2017,df_2018, df_2019,\n                            df_2020,df_2021)\n\n\nnew_column_names&lt;-c(medincome = \"B19013_001\",employment_status = \"B23025_001E\",\n  pct_below_poverty_level = \"B17001_002E\", #etc...\n)\n\n#rename the census return variables\ndf &lt;- combined_table %&gt;%\n  rename(\n    medincome = medincome,\n    no_english = B16004_005,\n    little_english = B16004_004,\n    has_highschool_diploma = B15003_017,\n    employment_status = B23025_001,\n    pct_below_poverty_level = B17001_002,\n    pct_homes_owner_occupied = DP04_0046P,\n    total_pop = B02001_002,\n    White_alone = B02001_003,\n    Black_or_African_American_alone = B02001_004,\n    American_Indian_and_Alaska_Native_alone = B02001_005,\n    Asian_alone = B02001_006, \n    Native_Hawaiian_and_Other_Pacific_Islander_alone\n    = DP04_0046P,\n    pct_25_and_up_bachelors_degree\n    = pct_25_and_up_bachelors_degree\n  )\n\ncolnames(df)\n\n#get original data set columns\nselected_columns &lt;- shooting_data[, c(\n  \"district_name\",\n  \"incident_num\",\n  \"Date\",\n  \"Time\",\n  \"district\",\n  \"victim_gender\",\n  \"victim_race\",\n  \"multi_victim\",\n  \"v_hispanic_or_latinx\",\n  \"fatal\",\n  \"Total.\"\n)]\n\n#add year column to shooting dataset \nselected_columns$year &lt;- year(as.Date(selected_columns$Date))\n\n\n\n\nzips&lt;-c(02135,02121, 02122, 02124, ...)\n\n#add district names corresponding to zipcodes/Geoid \n#for final merge with original dataset\nwith_districts &lt;- df %&gt;%\n  mutate(district_name = case_when(\n    GEOID == 2135 ~ \"Brighton\",\n    GEOID %in% c(2121, 2122, 2124, 2125) ~ \"Dorchester\",\n    GEOID == 2128 ~ \"East Boston\",\n    GEOID == c(2136,2137) ~ \"Hyde Park\",\n    GEOID == c(2130,2135) ~ \"Jamaica Plain\",\n    GEOID == 2126 ~ \"Mattapan\",\n    GEOID %in% c(2119,2120,2132) ~ \"Roxbury\",\n    GEOID == 2127 ~ \"South Boston\",\n    GEOID %in% c(2111, 2116, 2118, 2127) ~ \"South End\",\n    GEOID == 2132 ~ \"West Roxbury\",\n    TRUE ~ NA_character_\n  ))\n\n\n\nfinal_df&lt;-merge(selected_columns,with_districts, by =\n                  c('district_name','year'))\n\nwrite.csv(final_df, file = \"census_dat.csv\", row.names = FALSE)\n\n\ncolnames(final_df)"
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones and summarize the rest.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your load_and_clean_data.R file.\nRename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your load_and_clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nBlog 6: Thesis\n\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2024\n\n\n\n\n\n\n  \n\n\n\n\nBlog 5: Second Data Set\n\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2024\n\n\n\n\n\n\n  \n\n\n\n\nBlog 4: Data Modeling\n\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2024\n\n\n\n\n\n\n  \n\n\n\n\nBlog 3: Data Cleaning\n\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n  \n\n\n\n\nBlog 2: Data Background\n\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2024\n\n\n\n\n\n\n  \n\n\n\n\nBlog 1: Dataset Proposal\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\n\n\n\n\n  \n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post.\n\n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting.\n\n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Blog_6/Blog_6.html",
    "href": "posts/Blog_6/Blog_6.html",
    "title": "Blog 6: Thesis",
    "section": "",
    "text": "Our thesis is as follows: Shooting Frequency is negatively correlated with Income Per Capita. As IPC increases, Shooting Frequency decreases. We have initial graphs that support this thesis. We decided to use Income Per Capita rather than median income to normalize our values to account for differing population sizes. We are in the process of exploring the relationships between Shooting Frequency and Employment Status as well as Education. Our initial assumptions are that low employment is correlated with more shootings, and that low education (defined as having a high school diploma) is correlated with more shootings. In terms of additional exploration, we are in the process of generating graphs to either verify or refute our previous claims. We aim to investigate the relationship between socioeconomic factors and the rate of shooting incidents in the Boston area over a ten-year period from 2015 to 2024. Our goal is to understand how variables such as employment status, poverty levels, unemployment rates, median income, family structure, education levels, and homeownership rates influence the occurrence of shooting incidents across different neighborhoods. We predict that certain factors such as high unemployment rates and elevated poverty levels will correlate with increased shooting incidents in the Boston area from 2015 to 2024. Tables we might look into include using tidycensus to get visual representations of where the most police shootings occur. This will help verify whether our hypothesis of shooting frequency and location is consistent."
  },
  {
    "objectID": "posts/Blog_post_2/Blog_post_2.html",
    "href": "posts/Blog_post_2/Blog_post_2.html",
    "title": "Blog 2: Data Background",
    "section": "",
    "text": "Data Background\nData was collected by the Boston Regional Intelligence Center under the Boston Police Department Bureau of Intelligence and Analysis. This data includes both fatal and non-fatal shootings (shooting_type_v2), if a victim was struck by a bullet within the City of Boston that falls under the jurisdiction of the Boston Police Department (district). The data does not include shootings that were deemed justifiable or self-inflicted gunshot wounds. The dataset serves as a benchmark for BPD to analyze safety and violence in Boston districts and to assess the allocation of their focus and resources. It is likely collected with the aim of monitoring and responding to shooting incidents within the city. The data is from 2015 forward.\nPotential issues with this data could include inconsistencies in reporting across districts, missing or incomplete data, variation in how incidents are classified, and potential biases in reporting (underreporting in certain neighborhoods). Bias might also exist due to overrepresentation of certain demographics as victims or systemic biases within law enforcement practices.\nThis data could be used for various purposes including trend analysis over time, understanding spatial distribution across districts, examining victim demographics, and exploring relationships between shooting incidents and victim characteristics. There may have been prior research utilizing this data to understand patterns of gun violence in Boston or to inform policy decisions related to public safety. The data might inform policy decisions related to allocation of resources for crime prevention, community outreach efforts, or gun control measures. Questions from policymakers might include inquiries about trends in shootings, effectiveness of intervention programs, or factors contributing to high-risk areas."
  },
  {
    "objectID": "posts/Blog_4/blog4.html",
    "href": "posts/Blog_4/blog4.html",
    "title": "Blog 4: Data Modeling",
    "section": "",
    "text": "Two graphs we have emphasized are the relationship between victim and gender and incidents by district. Using these two graphs will help emphasize where Boston shootings most occur, and who is most at-risk in getting involved. Predictor variables we may be interested in are the district numbers and gender; these two variables are interesting to see where the greatest number of crimes occur - to observe possible patterns and for forecasting.\nOur analysis so far has highlighted gender differences and the frequency of incidents by district without exploring how these factors may interact with others. We could further explore temporal patterns to shed light on when incidents are more likely to occur. We could also focus on outcomes, particularly the distinction between fatal and non-fatal incidents, to help us understand the factors that contribute to the severity of shootings.\nBased on our dataset, there are two potential directions we can take for the choice of the response variable. We could predict the severity of incidents (whether it was fatal) or predict the frequency of incidents based on various factors (time, location, demographic information). To predict incident frequency, we would need to aggregate the data to have counts per day, district, or another relevant grouping. Possible predictor variables could include date/time for temporal trends, district name/district for geographical influences, victim race/gender for demographic factors of victims, or socioeconomic factors like income. If our response variable is the severity of incidents, logistic regression is the most appropriate for predicting the possibility of an incident being fatal based on the predictors. Linear regression could be used if our response variable is the number of incidents to fit a continuous distribution."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/Blog_3/Blog_3.html",
    "href": "posts/Blog_3/Blog_3.html",
    "title": "Blog 3: Data Cleaning",
    "section": "",
    "text": "Blog Post 3: Data Cleaning\nWe first used View() to get a preliminary view of the data. We found that some of the column names and values were overly complicated and could be simplified and recategorized. For example, the column “victim_ethnicity_NIBRS” contained the values “Hispanic or Latinx” and “Not Hispanic or Latinx”. This was simplified into a “v_hispanic”or_latinx” column and the values were replaced with Booleans. The “shooting_type_v2” column (values: “fatal”, “not fatal”) were reformatted into the “fatal” column and Boolean values as well.\nThe “district” column that came with the dataset only used district codes (“A7” for East Boston, “B2” for Roxbury, etc.). A list of Boston Districts and their respective codes was created with information from: https://police.boston.gov/districts/. This information was then used to create a new column “district_name” that contains the string name of the district where the shooting occurred. It is worth noting that on the website listed, A1 and A15 are listed as “A1 & A15”, “Downtown & Charlestown”; However, due to various other sources listing Downtown and Charlestown as separate districts, we separated them and mapped “A1” to “Downtown” and “A15” to “Charlestown”. Boston Police Department consistently refers to “A1” and “A15” together however, so we may remerge them in our dataset later.\nWe searched for duplicate columns and initially thought we had found them as multiple rows contained the same “incident_num”. However, upon further analysis, we found that shootings with multiple victims were split into multiple rows, as each row would correspond to each individual victim. Therefore, it would be incorrect to remove duplicates based on incident numbers. This conclusion is corroborated by the “multi_victim” column, which is true for rows in which the incident number is shared with other rows.\nIn the initial data set, date and time were combined in one column and they were both string values. To clean this column, we split the date and time into separate columns and modified the data types using the as.Date function and changed the time to match the format “%H:%M:%S” . Since the initial data set lacked demographic information for each district, we downloaded 2020 census data for each district and merged the data into our final cleaned data set. The demographic data columns of interest are household size, College/University student housing, Institutionalized population, Black, White, Hispanic or Latino, Asian, Native Hawaiian and Pacific Islander, and Other. The demographic data set did not include the district of Charlestown & Downtown resulting in a loss of 30 rows.\n\n\n\n\n\n\n\nData for Equity\nBeneficence requires maximizing benefits while minimizing harm. Applying this principle to the data involves critically evaluating the risks and benefits of collecting, analyzing, and sharing information about shootings. For example, it entails carefully considering how the data could be used to improve public safety and community well-being without reinforcing negative stereotypes about certain communities. To adhere to this principle, data should be aggregated or anonymized while protecting individuals’ identities, while still providing valuable insights for public safety efforts. Transparency is also crucial in maintaining the trust of the community from which the data was collected. It involves being clear about the collection methods, the purpose of the data, and how it will be used. Transparency also requires acknowledging the limitations of the analysis, such as potential biases in data collection (over or under reporting in certain areas) and the interpretations made from the data. Limitations in the analysis could include potential biases in how incidents are reported or recorded, leading to an incomplete or skewed understanding of crime patterns. Additionally, the data could also be misused to unfairly target specific communities."
  },
  {
    "objectID": "posts/Blog_5/Blog_5.html",
    "href": "posts/Blog_5/Blog_5.html",
    "title": "Blog 5: Second Data Set",
    "section": "",
    "text": "Our team will be modeling the rate of shooting incidents in the Boston area from 2015-2024, and to build an accurate model, our team will be analyzing socioeconomic indicators to draw meaningful conclusions. To obtain variables that we think will have significant interactions with the outcome variable, we have decided to utilize the tidy census package to query data from the American Community Survey through Census.gov tables. Specifically, we are considering key indicators such as employment status, percentage of those above the poverty level, unemployment rate, median income, percent of single-parent households, high school graduate population, and percentage of homes that are owner occupied. To introduce spatial analysis, we have pulled demographic and geolocation data for each of the years to analyze the distribution of incidents across race in key Boston districts based on zip codes. The names of each variable and their primary key in the database are listed below. \nAfter querying the relevant data, the next steps our team will take will involve merging the tables for each year with our initial data set. We plan on joining these tables based on year and zip code keys. Given that we are covering spatial data, we would like to model statistics such as diversity gradient and dissimilarity index across the different regions where incidents take place. We will also be producing map plots to demonstrate the difference in our dependent variable across the different regions of coverage. Additionally, the data is distributed over time, so our team will be able to analyze the temporal variance of incidents with a time series analysis. \nLink to R script with query code: https://github.com/sussmanbu/final-project-team1/blob/d0bbc5e22b53cb188ed9c0fa22820c64e190a2b6/tidycensus.R\nRace variables:  White alone, Black alone, American Indian and Alaska Native alone, Asian alone, Native Hawaiian and Other Pacific Islander alone\nVariable IDs:\n‘DP03_0009’  unemployment rate\n‘B19013_001’ median income \nB11005_003E single parent households\n‘B16004_005E’ percent of individuals who do no speak english at all \n‘B16004_004E’ percent of individuals who do not speak english well \n‘B15003_017E’ high school graduate population \n‘C24010_’  Occupation by industry \n‘B23025_001E’ Employment status \n‘B17001_002E’  percent of individuals below poverty level \n‘B17001_003E’ percent above poverty level \n‘DP04_0046PE’  percent of homes that are owner occupied"
  },
  {
    "objectID": "posts/2024-02-28-first-blog-post/first-blog-post.html",
    "href": "posts/2024-02-28-first-blog-post/first-blog-post.html",
    "title": "Blog 1: Dataset Proposal",
    "section": "",
    "text": "Dataset 1 - Analyze Boston Shootings\nLink: https://data.boston.gov/dataset/shootings/resource/73c7e069-701f-4910-986d-b950f46c91a1?inner_span=True\nThis dataset from Analyze Boston contains information about shootings in Boston, comprised of 1925 rows and 8 columns, labeled “incident_num”, “shooting_date”, “district”, “shooting_type_v2”, “victim_gender”, “victim_race”, “victim_ethnicity_NIBRS”, and “multi_victim”. The original collection of this data might have been driven by law enforcement or public safety organizations aiming to monitor and respond to shooting incidents within the city. The inclusion of details like shooting type, victim demographics, and whether an incident involved multiple victims suggests an intent to understand patterns in gun violence, the demographic distribution of victims, and potentially identify areas or populations at higher risk. Loading and cleaning the data is possible, and initial steps could involve identifying any missing values. The main questions this dataset could address might revolve around identifying trends in shootings over time, assessing the distribution of shootings across different districts, understanding the demographic profile of victims, and analyzing the relationship between the type of shooting incidents and victim demographics. Challenges in analyzing this dataset might include dealing with missing or incomplete data.\nDataset 2 - Tobacco Use\nLink: https://catalog.data.gov/dataset/behavioral-risk-factor-data-tobacco-use-2011-to-present\nThis data set is from the U.S. Department of Health and Human Services. This data comes from the summation of several Tobacco surveys from 2011 to 2019. It contains 15 columns after cleaning and 43341 rows detailing location, year, race, type of tobacco consumption, gender, race, age, education, and more. This data could help us understand what demographics and locations have worse tobacco use habits, and how different age groups as well as levels of education may have different trends in tobacco use. Problems may arise from using this data set if different categories have different amounts of data, it may be harder to compare trends between categories.\nDataset 3 - School Attendance\nLink : https://catalog.data.gov/dataset/school-attendance-by-student-group-and-district-2021-2022\nThis dataset analyzes public school attendance from Pre-K to twelfth grade. This range will provide us with information on how students’ backgrounds like race or income level affects whether a student attends school regularly. There are 2020 observation rows. Furthermore, because the data is separated based on district, it will be clearer on how attendance might change based on demographics. Furthermore, the dataset analyzes attendance rate as a proportion to total student count on three date ranges - 2019 to 2020; 2020 to 2021; and 2021-2022. Finally, because each school district’s attendance rate is further separated by different “categories,” like race or whether or not they qualify for “Free/Reduced Lunch,” it will be useful to analyze how different hardships that students face may correlate with attendance rate. A challenge with using this dataset might be deciding which categories to use and how we can use the categories to emphasize the most meaningful results."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team TEAMNAME. The members of this team are below."
  },
  {
    "objectID": "about.html#hirotaka-caden-fuiji",
    "href": "about.html#hirotaka-caden-fuiji",
    "title": "About",
    "section": "Hirotaka Caden Fuiji",
    "text": "Hirotaka Caden Fuiji\nHiro is an undergraduate student majoring in Data Science. His GitHub Page is https://github.com/fujiihc ## John Markowicz\nJohn is an undergraduate student majoring in Data Science. His GitHub page is https://github.com/jcmarkowicz."
  },
  {
    "objectID": "about.html#alexandra-rodriguez",
    "href": "about.html#alexandra-rodriguez",
    "title": "About",
    "section": "Alexandra Rodriguez",
    "text": "Alexandra Rodriguez\nAlexandra is an undergraduate student majoring in Computer Science and Psychology. Her Github page is https://github.com/lexi0001."
  },
  {
    "objectID": "about.html#nathan-rosenblum",
    "href": "about.html#nathan-rosenblum",
    "title": "About",
    "section": "Nathan Rosenblum",
    "text": "Nathan Rosenblum\nNathan is a undergraduate senior studying Applied Mathematics, his github page is https://github.com/nrosenblum."
  },
  {
    "objectID": "about.html#eric-yang",
    "href": "about.html#eric-yang",
    "title": "About",
    "section": "Eric Yang",
    "text": "Eric Yang\nEric is an undergraduate student in Economics with a minor in History. His Github page is https://github.com/ericyang02."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.Rmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this pge should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nInteractive\nYou will also be required to make an interactive dashboard like this one.\nYour Big Data page should include a link to an interactive dashboard. The dashboard should be created either using Shiny or FlexDashboard (or another tool with professor’s approval). This interactive component should in some way support your thesis from your big picture page. Good interactives often provide both high-level understanding of the data while allowing a user to investigate specific scenarios, observations, subgroups, etc.\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions from the Big Picture? Plotly with default hover text will get no credit. Be creative!\n\n\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  }
]