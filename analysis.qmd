---
title: Analysis
description: Here we provide a detailed analysis using more sophisticated statistics techniques.
toc: true
draft: false
---

![](https://upload.wikimedia.org/wikipedia/commons/7/77/Pebbleswithquarzite.jpg)

This comes from the file `analysis.qmd`.

We describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You'll also reflect on next steps and further analysis.

The audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.

While the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.

The style of this paper should aim to be that of an academic paper. I don't expect this to be of publication quality but you should keep that aim in mind. Avoid using "we" too frequently, for example "We also found that ...". Describe your methodology and your findings but don't describe your whole process.

### Example of loading data

The code below shows an example of loading the loan refusal data set (which you should delete at some point).


library(tidyverse)
print(getwd())
data <- read_csv(here::here("dataset/loan_refusal_clean.csv"))
load(here::here("dataset/loan_refusal.RData"))
print(ls())


## Note on Attribution

In general, you should try to provide links to relevant resources, especially those that helped you. You don't have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don't need a formal citation.

If you are directly quoting from a source, please make that clear. You can show quotes using `>` like this

```         
> To be or not to be.
```

> To be or not to be.

------------------------------------------------------------------------

## Rubric: On this page

You will

-   Introduce what motivates your Data Analysis (DA)
    -   Which variables and relationships are you most interested in?
    -   What questions are you interested in answering?
    -   Provide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.
-   Modeling and Inference
    -   The page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.
    -   Explain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)
    -   Describe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.
-   Explain the flaws and limitations of your analysis
    -   Are there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?
-   Clarity Figures
    -   Are your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?
    -   Each figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)
    -   Default `lm` output and plots are typically not acceptable.
-   Clarity of Explanations
    -   How well do you explain each figure/result?
    -   Do you provide interpretations that suggest further analysis or explanations for observed phenomenon?
-   Organization and cleanliness.
    -   Make sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.
    -   This page should be self-contained, i.e. provide a description of the relevant data.
    
    
```{r}

#analysis summary: focus on R-squared/adjusted R-squared, our models capture a majority of the variance in our dependent variable, total cases. Therefore, our choice of variables are good indicators and have substantial predictive power for the number of incidents in a given district. 
#Outline: 
#summary statistics: correlations of our independent variables with total cases, correlation table seems intuitive, for example as unemployment increases, total cases also increase. Show the distribution of our variables, include more creative plots for appeal, reference tidycensus,Rfor data science books. 

#Modeling: good R-Squared, above 90 for adjusted R-squared which is key, look at the relationship across spatial, socio economic, and racial features, geospatial analysis to weight the proximity of our locations, plot Beta, where is our regressors more impactful?, compare to segregation index and map. 

library(tidycensus)
library(tidyverse)
library(sandwich)
library(lmtest)
library(car)
library(leaps)
library(segregation)
library(tigris)
library(sf)
library(olsrr)
library(GWmodel)
final_df<-readRDS('all_census_dat.rdr')


#correlation table for all independent variables
independent_dat<-final_df%>%
  group_by(district_name)%>%
  summarise(aian_pct = mean(American_Indian_and_Alaska_Native_alone/total_pop),
            nhpt_pct = mean(Native_Hawaiian_and_Other_Pacific_Islander_alone/total_pop), 
            asian_pct = mean(Asian_alone/total_pop),
            pct_black = mean(Black_or_African_American_alone/total_pop),
            pct_white = mean(White_alone/total_pop),
            unemployment=mean(not_in_labor_force/total_pop),
            medincome_mean= mean(household_medincome),
            bachelors = mean(bachelors_25/total_pop),
            one_parent= mean(one_parent/total_house_units),
            married_house= mean(married_households/total_house_units),
            vacant_houses = mean(vacancy_status/total_house_units),
            total_cases = sum(!duplicated(incident_num)))%>%
  select(-district_name,-total_cases)

dependent_dat<-final_df%>%
  group_by(district_name)%>%
  summarise(total_cases = sum(!duplicated(incident_num)))

correlation_coeffs <- cor(dependent_dat$total_cases, independent_dat)

variable_names <- colnames(independent_dat)
correlation_values <- as.vector(correlation_coeffs)

correlation_df <- data.frame(
  Variable = variable_names,
  Correlation_Coefficient = correlation_values
)

print(correlation_df)

```


```{r}
#time to work plot 
final_df%>%
  group_by(district_name)%>%
  na.omit(aggregate_time_to_work)%>%
  summarise(work_time= mean(aggregate_time_to_work), total_cases = sum(!duplicated(incident_num))) %>%
  ggplot(aes( work_time,total_cases))+
  geom_point()+
  geom_smooth(method ='lm')+
  labs(y = 'Avg travel time to work',title = 'Avg Travel Time to Work and Total Cases')

```


```{r}
#income plot
final_df%>%
  group_by(district_name)%>%
  summarise(medincome_mean= mean(household_medincome), total_cases = sum(!duplicated(incident_num))) %>%
  ggplot(aes(district_name, medincome_mean,fill = total_cases))+
  geom_bar(stat= 'identity')+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+
  labs(y = 'Avg median income',title = 'Avg Median Income and Total Cases Over All Years')

```

```{r}
#single parent plot 
final_df%>%
  group_by(district_name)%>%
  summarise(single_household = mean(one_parent/ total_house_units),total_cases = sum(!duplicated(incident_num))) %>%
  ggplot(aes(district_name,single_household, fill = total_cases))+
  geom_bar(stat= 'identity')+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+
  labs(title = 'Pct of Households with Single Parent')

```


```{r}
#ols all variables 

lm_dat<-final_df%>%
  group_by(district_name, year)%>%
  summarise(aian_pct = mean(American_Indian_and_Alaska_Native_alone/total_pop),
            nhpt_pct = mean(Native_Hawaiian_and_Other_Pacific_Islander_alone/total_pop), 
            asian_pct = mean(Asian_alone/total_pop),
            pct_black = mean(Black_or_African_American_alone/total_pop),
            pct_white = mean(White_alone/total_pop),
            unemployment=mean(not_in_labor_force/total_pop),
            medincome_mean= mean(household_medincome),
            bachelors = mean(bachelors_25/total_pop),
            one_parent= mean(one_parent/total_house_units),
            married_house= mean(married_households/total_house_units),
            vacant_houses = mean(vacancy_status/total_house_units),
            travel_work = mean(aggregate_time_to_work),
            total_cases = sum(!duplicated(incident_num)))

#OLS with all variables 
ols<-lm(total_cases~travel_work+ pct_white+pct_black+asian_pct+nhpt_pct+aian_pct+district_name+ unemployment+ medincome_mean+bachelors+one_parent+ vacant_houses+married_house, data = lm_dat)
coeftest(ols, vcov. = vcovHC) # data is heteroskedastic, opted to use the heteroskedastic robust standard errors 
summary(ols) 
vif(ols) #variables highly correlated, high pvalues, sign of coefficients opposite of their correlation,where you live is the best predictor of the total number of cases, 

ols_step_forward_p(ols)
#attempted variable selection, all the districts were retained, hence their predictive power, travel_work, income, married households also selected based on their significance, 

```


```{r}
#socio economic and race 

ols<-lm(total_cases~ asian_pct+pct_white+pct_black+nhpt_pct+aian_pct+travel_work+unemployment+medincome_mean+bachelors+one_parent+married_house, data = lm_dat)
coeftest(ols, vcov. = vcovHC)
vif(ols)
ols_step_forward_p(ols) # medium income,bachelors,pct_asian, pct_white significant 5% level, one_parent,travel_work at 10%

#coefficeint signs alot more interpretable and align with correlation direction, 

#probably should add one more regression with interaction terms 

```


```{r}

#map segregation index, relied on tidycensus book 

race_data<-c("White_alone", "Black_or_African_American_alone",
             "American_Indian_and_Alaska_Native_alone",
             "Asian_alone", "Native_Hawaiian_and_Other_Pacific_Islander_alone")


pivot_race<-final_df%>%
  group_by(year, GEOID_TRACT_20)%>%
  pivot_longer(race_data,names_to ='ethnicity',values_to = 'population_count')%>%
  ungroup()%>%
  group_by(ethnicity, GEOID_TRACT_20)%>%
  summarise(mean_population_count = mean(population_count))

print(pivot_race, n =100)

boston_local_seg <- pivot_race %>%
  mutual_local(
    group = "ethnicity",
    unit = "GEOID_TRACT_20",
    weight = "mean_population_count", 
    wide = TRUE
  )%>%
  rename(GEOID = GEOID_TRACT_20)

boston_local_seg$GEOID<-as.character(boston_local_seg$GEOID)

MA_tracts_seg <- tracts("MA", cb = TRUE, year = 2020) %>%
  inner_join(boston_local_seg, by = "GEOID") 

with_geometry <- get_acs(
  geography = "tract",
  variables = c(total_house_units = 'B25001_001'),
  state = "MA",
  geometry = TRUE,
  year = 2021
)

MA_tracts_seg%>%
  ggplot(aes(fill = ls)) + 
  geom_sf(color = NA) + 
  coord_sf(crs = st_crs(with_geometry))+
  scale_fill_viridis_c(option = "inferno") + 
  theme_void() + 
  labs(fill = "Local\nsegregation index")


#compare this to geospatial weighted regression maps 

```


```{r}

#Geography weighted Regression 

#Implemented with the GWmodel R package and spgwr package that offer a methods for a geographical robust weighted regreession
# The GWR model computes a local regression model for each location, while including a distance decay function, or a weight added to sum of linear combinations which specifies how observations outside the current location will be weighted
# relative to their distance. A kernel bandwith is implemented to determine the cutoff distnace for observations that will be included for a locations set of linear combinations
# The GWR model returns local parameter estimates, and the local R-squared quatifying how much variance was captured in a given area. We plot the R-squared statistic for each location in our data below
#


#best data to run this is just socio economic,
ols<-lm(total_cases~ unemployment+medincome_mean+bachelors+one_parent+ vacant_houses+married_house, data = lm_dat)
coeftest(ols, vcov. = vcovHC)
summary(ols)
vif(ols)
ols_step_forward_p(ols) #variable selction, use the subset, look at statistically significant variables, unemployment has good predictive power  



geo_dat<-final_df%>%
  group_by(district_name,year)%>%
  summarise(aian_pct = mean(American_Indian_and_Alaska_Native_alone/total_pop),
            nhpt_pct = mean(Native_Hawaiian_and_Other_Pacific_Islander_alone/total_pop), 
            asian_pct = mean(Asian_alone/total_pop),
            pct_black = mean(Black_or_African_American_alone/total_pop),
            pct_white = mean(White_alone/total_pop),
            unemployment=mean(not_in_labor_force/total_pop),
            medincome_mean= mean(household_medincome),
            bachelors = mean(bachelors_25/total_pop),
            one_parent= mean(one_parent/total_house_units),
            married_house= mean(married_households/total_house_units),
            vacant_houses = mean(vacancy_status/total_house_units),
            travel_work = mean(aggregate_time_to_work),
            total_cases = sum(!duplicated(incident_num)),
            geometry = geometry)%>%
  distinct()

geo_reg<-st_as_sf(geo_dat)%>%
  as_Spatial()

#limitation: travel_work has na values, cannot include this in model, even though it would be great to have that plotted 

#other variables we should add to aid analysis?

formula2 <- "total_cases~ unemployment+ vacant_houses+medincome_mean+bachelors+one_parent+married_house"

bw <- bw.gwr(
  formula = formula2, 
  data = geo_reg, 
  kernel = "bisquare",
  adaptive = TRUE
)

gw_model <- gwr.basic(
  formula = formula2, 
  data = geo_reg, 
  bw = bw,
  kernel = "bisquare",
  adaptive = TRUE
)

gw_model_results <- gw_model$SDF %>%
  st_as_sf() 

#map for R^2
ggplot(gw_model_results, aes(fill = Local_R2)) + 
  geom_sf(color = NA) + 
  scale_fill_viridis_c() + 
  theme_void()

#great R-squared and all of our variables are statistically significant at least 10% level, this is good

#map for beta, choose the fill variable/beta coefficient 

#where is the effect positive and negative, 
ggplot(gw_model_results, aes(fill = unemployment)) + 
  geom_sf(color = NA) + 
  scale_fill_viridis_c() + 
  theme_void() + 
  labs(fill = "Local β Unemployment")





```